# RESOURCE SHARING

- DEV
- QA
- PROD

## CHALLENGE 1
### How do DevOps Engineer allocate resources for differenet env's

![image](https://github.com/pavankumar0077/kubernetes-troubleshooting-zero-to-hero/assets/40380941/82b6a187-e76e-4373-8b59-0222560471d7)

- Let's say we have **E-COMMERECE** application and we have different teams like
- 1. Web application dev
  2. Payment tema
  3. Transactions
  4. DB
  5. API 
- Simple answer is creating a LOGICAL SEPEARTION -- NAMESPACES.
- The challenge is Let's assume all the NS has WORKER NODES, TOTATL WE HAVE 100 CPU's & 100 RAM mib
- Assume we have 5 teams and 5 NS we have created each worker has 20 GB OF CPU AND RAM ( Usually it will be 32gb or 64gb of ram )
- Teams started deploying their services in their NAMESPACE.
- One service or some services in the payment Namespace are LEAKING MEMEORY ( THEY ARE CONSUMING MEMROY MORE THAN REQUIRED FOR SOME REASONS )
- Might application issue or perrformance testing is not done properly, SO IT STARTED USING MOST OF THE RESOURCES IN THE CLUSTER
- USUALLY IT SHOULD TAKE 2GB OF RAM, BUT IT STARTED TAKING MORE RESOUCES LIKE 32 GB RAM.
- DUE TO THIS REASON THE OTHER TEAMS OTHER SERVICES ARE GETTING LESS RESOURCES FROM THE CLUSTER,
- LET'S SAY PAYEMNT NS is TAKING 40GB OF RAM, THEN ALL THE OTHER RESOUCES WILL TAKE 60GB OF RAM FOR REST OF THE 4 NAMESPACE WHICH IS VERY LESS COMPARE TO ACUTAL.
- BECAUSE OF THIS IN API TEAM, SERVICES WILL NOT GET SUFFICENT RESOURCES MIGHT GET CRASHED. MIGHT GET INTO CRASHLOOPBACKUP DUE TO OOMKILLED ISSUE.
- **THIS IS CAN TO FIXED BY PROVIDING PARTICULAR RESOUCES QUOTA LIMIT TO THE NAMESPACE - USING RESOURCE QUOTA**

![image](https://github.com/pavankumar0077/kubernetes-troubleshooting-zero-to-hero/assets/40380941/d8f672ec-2cdf-48f5-8710-95e6e037ef17)


### SOLUTION - PART 1 - USE RESOURCE QUOTA
- FOR EACH NS -- WE HAVE USE RESOURCE QUOTA
- **HERE WE ARE RESTRICTING FOR PARTICULAR NS**
- IT IS A LIMIT THAT WE APPLY FOR NS
- for example if we set RESOUCE QUOTA TO 15GB FOR NS, Any no of the services in the NS should take 15GB of RAM.
- We can set with respect to CPU AND RAM both
- AS A DEVOPS ENGINEER WE WILL INTERACT WITH DEV TEAMS LIKE WHAT IS THE REQUIRED AMOUNT OF RESOURCES FOR ALL YOUR MICROSERVICES TO RUN
- HOW DEV TEAMS WILL SAY THEY CAN'T TELL DIRECTLY 20 GB, 40 GB LIKE THAT, THEY HAVE TO DO **PERFORMANCE BENCHMARKING** BY USING PERFORMANCE BENCHMARKING THEY HAVE TO COME UP WITH IDEAL NUMBER
- THEN THEY WILL COME UP WITH LIKE WE NEED 20GB OF RAM AND 20GB OF CPU for this NAMESPACE
- NOW EACH NS WILL GET DIFFERNET DIFFERENT RESOURCES QUOTA Like NS 1 required 20GB, NS 2 may required only 10GB.
- ALL THE NAMESPACES SHOULD NOT EXCEED 100GB, IF DEV TEAMS ARE ASKING MORE AND THEY DID PEROFRMANCE BEANCHMARKING AS PER ORG STANDARDS THEY WE HAVE TO ALLOCATED THEM RESOURCE BY SCALING THE CLUSTER
- PROBABLY WE HAVE TO ADD OTHER WORKER NODE AND MAKE SURE ALL THE SERVICES ARE RUNNING SMOTHLY.

![image](https://github.com/pavankumar0077/kubernetes-troubleshooting-zero-to-hero/assets/40380941/108e2895-0721-4a63-9ee3-ec5e9c3a9482)

### SOLUTION - PART 2 - USE RESOURCE LIMITS
- 15 CPU's and 15 GB RAM for WEB APP - NS, Which is running 5 SERVICES, ONE PARTICULAR MICROSERVICE LEAKING MEMORY
- PERVIOSULY IT IMPACTED THE ENTIRE CLUSTER, NOW IT IS IMPACTING THE PARTICULAR NAMESPACE
- **WE HAVE TO SETUP RESOURCE, REQUESTS OR RESOURCE LIMITS**
- **RESOURCE QUOTE FOR CLUSTER, RESOURCE LIMIT IS FOR PARTICULAR POD**
- **NOW WE ARE RESTRICT RESOURCE ON A PARTICULAR POD**
- AS A DEVOPS ENGINEER WE WILL GO TO DEV TEAMS AND TAKE PERFORMANCE BENCHMARKING FOR EACH APPLIICATION,
- LET SAY EXAMPLE IF WE HAVE ALLOCATED THE MEMORY AND CPU AFTER THE BENCHMARKING BUT STILL IT IS APPLICATION IS CAUSING AN ISSUE AND IMPACTING THE OTHER APPLICATIONS, THEN DEV TAEM HAS TO HANDLE THAT.

![image](https://github.com/pavankumar0077/kubernetes-troubleshooting-zero-to-hero/assets/40380941/6ee705ae-e4e2-4af8-93c2-b052ef925654)

## CHALLENGE 2
### OOM KILLED ISSUES WITH POD
- ASSUME THE APPLICATION IS JAVA MICROSERVICE APPLICATION
- **WE HAVE TO SHARE THE THREAD DUMP & HEAP DUMP TO DEV TEAMS**
- **THREAD DUMP IS TAKEN USING Kill -3 command**
- **IF we execute Jstack command we will get HEAP DUMP.**
- THIS IS FOR JAVA APPLICATION
- DEV TEAMS WILL ANALYSIS THE THREADS AND FIND THE RCA FOR IT AND COME UP WITH NEW VERSION OF THE APPLICAITON

![image](https://github.com/pavankumar0077/kubernetes-troubleshooting-zero-to-hero/assets/40380941/d1ae6e31-30a6-4721-9c88-384ef316450e)

## CHALLENGE 3
### UPGRADES
- MANUAL IN DETAILED
- BACKUP
- RELEASE NOTES
- DIVIDED STEP FOR CONTROL PLANE COMPONENTS -- ETCD, API SERVER, SCHEDULOR
- DEVIDED STEPS FOR WORKER PLANE COMPONRNTS (DATA PLANE) --
- IF WE HAVE 3 WORKER NODES - THEN 1ST WE HAVE TO DRAIN THE NODE, IT MEANS WE ARE GIVING SCHUDULOR TO MOVE THE PODS TO ANOTHER NODE
- ONCE THE NODE IS EMPTY ALL PODS ARE MOVED TO ANOTHER NODE THEN YOUR NODE WILL BECOME UNSCHULABLE **TRAINT**   DISCONNECT THE NODE AND UPGRADE THE KUBELECT INSTALL THE NEW PACKAGES
- THEN WE WILL BRING THE NODE UPDATE AND JOIN THE NODE AGAIN BACK TO THE CLUSTER. WE WILL REMOVE UNSCHDULAABLE TRAINT AND OTHER TRAINT ON THE UPGRADED NODE
- DO THE SAME FOR OTHER NODES AS WELL.
